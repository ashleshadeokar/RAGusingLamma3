# RAGusingLamma3

We are building a self-hosted "Chat with your Docs" application using the powerful Llama-3 language model. This project leverages Meta AI's Llama3 as the Language Model (LLM) and combines it with other key components to provide an interactive experience for querying documents.

![Alt Text](https://github.com/ashleshadeokar/RAGusingLamma3/blob/main/RAG.png)

## Overview
Llama3 is recognized as one of the most capable openly available Large Language Models (LLMs) to date. In this project, we have developed a RAG (Retrieval Augmented Generation) application, allowing users to interact with their documents through natural language queries.

## Key Components

1. LlamaIndex
LlamaIndex serves as the orchestration layer, facilitating the interaction between the user queries and the documents in the repository. It plays a crucial role in organizing and retrieving relevant information.

2. Streamlit
Streamlit is utilized for creating a user-friendly Chat UI. This component provides an intuitive interface for users to input queries and interact with the RAG application seamlessly.

3. Meta AI's Llama3
Llama3, developed by Meta AI, serves as the core Language Model (LLM) powering the RAG application. With its advanced natural language processing capabilities, Llama3 enables the generation of responses based on the user's queries.

## Usage
To get started with RAGusingLlama3, follow these steps:

- Clone the repository to your local machine.
- Install the necessary dependencies.
- Run the `main.ipynb` notebook, which contains the essential code for setting up the RAG application.
- Interact with your documents using the provided Chat UI created with Streamlit.

## Conclusion
In conclusion, RAGusingLlama3 represents a significant achievement in the development of Retrieval Augmented Generation (RAG) applications. By leveraging Llama3, LlamaIndex, and Streamlit, we have created a robust solution for interacting with documents through natural language queries.
